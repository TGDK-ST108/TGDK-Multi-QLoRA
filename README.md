# TGDK-QLoRA with Duo

ğŸš€ **TGDK-QLoRA** is the worldâ€™s first **multi-model symbolic QLoRA variant**, extending the Hugging Face QLoRA technique into TGDKâ€™s sovereign, license-bound ecosystem.

This release demonstrates how **BERT** (understanding) and **Mistral** (generation) can be fine-tuned together under **TGDKâ€™s Duo arbitration layer**, with differentiated optimizers (**AdamW** and **Lion**) applied for maximum efficiency.

---

## âœ¨ Key Features
- **Multi-Model Training** â†’ run **BERT + Mistral** in a single fine-tuning pipeline.  
- **Optimizer Differentiation** â†’ AdamW for stability in BERT; Lion for fast convergence in Mistral.  
- **Duo Orchestration** â†’ a TGDK interface that dynamically routes optimizer updates across models.  
- **Hybrid Licensing** â†’ this package is released under the TGDK-BFE Research License (see [LICENSE](LICENSE.txt)).

---

## ğŸ§© Project Structure
```
â”œâ”€â”€ qlora.py # Training wrapper with multi-model support
â”œâ”€â”€ Duo.py # AdamW + Lion selection logic
â”œâ”€â”€ duo_interface.py # Public Duo arbitration interface (no internals)
â”œâ”€â”€ train_config.yaml # Example training configuration
â”œâ”€â”€ README.md # You are here
â””â”€â”€ examples/
â”œâ”€â”€ run_mistral.sh # Fine-tune Mistral with Lion
â””â”€â”€ run_bert.sh # Fine-tune BERT with AdamW
```

## âš™ï¸ Usage


AdamW â†’ BERT (encoder stability, clause fidelity).

Lion â†’ Mistral (generative speed, entropy-aware loops).

âš ï¸ Note: This repo exposes only the public Duo interface. Full Duo internals, VaultLedger hooks, and TGDK sealing remain proprietary.
## Duo Hybrid vs. Baseline (QLoRA, 7B)

| Metric | Baseline (AdamW/Lion std) | Duo Hybrid (Duo + Jade + MMT) |
|--------|----------------------------|-------------------------------|
| Train loss (final) | 4.06 | 4.06 |
| Eval loss (final)  | 4.47 | 4.47 |
| Train runtime      | 6021.5 s | **4262.1 s** |
| Train samples/sec  | 1.36 | **1.92** |
| Train steps/sec    | 0.042 | **0.059** |
| Eval runtime       | 265.15 s | **172.01 s** |
| Eval steps/sec     | 1.003 | **1.546** |
| Grad norm (peak â†’ final) | ~15.6 â†’ ~12.3 | ~13.9 â†’ **~11.0** |
| LR schedule (end)  | linear (2e-5) | linear (2e-5) + Jade/Plateau escape |


Takeaways

~30% faster end-to-end training (1.36 â†’ 1.92 samples/s) with Duo Hybrid.

Smoother optimization: lower final grad norm (~12.3 â†’ ~11.0).

No loss regression: train/eval losses match baseline parity.

Eval pass faster (265s â†’ 172s), same batch/seq settings.

Representative logs

Baseline (final):
train_loss=4.0628 | eval_loss=4.4685 | train_runtime=6021.5s | samples/s=1.36 | steps/s=0.042 | eval_steps/s=1.003

Duo Hybrid (final):
train_loss=4.0628 | eval_loss=4.4685 | train_runtime=4262.1s | samples/s=1.92 | steps/s=0.059 | eval_steps/s=1.546

Repro (Duo Hybrid)
```
python qlora.py \
  --optimizer duo \
  --duo-mode hybrid \
  --epochs 6 \
  --learning_rate 2e-5 \
  --weight_decay 0.01 \
  --warmup_steps 200 \
  --scheduler_type linear \
  --use-mmt \
  --use-jade \
  --plateau-patience 150 \
  --plateau-delta 5e-4
```

## ğŸ“œ License
This release is provided under the TGDK-BFE Research License.

Free for research, academic, and non-commercial use.

Commercial deployment requires a TGDK license and VaultLedger binding.

ğŸ“¡ Credits
Developed by Sean Tichenor (TGDK LLC) with OliviaAI orchestration.
Part of the TGDK Sovereign AI Ecosystem.
