# TGDK-QLoRA with Duo

ğŸš€ **TGDK-QLoRA** is the worldâ€™s first **multi-model symbolic QLoRA variant**, extending the Hugging Face QLoRA technique into TGDKâ€™s sovereign, license-bound ecosystem.

This release demonstrates how **BERT** (understanding) and **Mistral** (generation) can be fine-tuned together under **TGDKâ€™s Duo arbitration layer**, with differentiated optimizers (**AdamW** and **Lion**) applied for maximum efficiency.

---

## âœ¨ Key Features
- **Multi-Model Training** â†’ run **BERT + Mistral** in a single fine-tuning pipeline.  
- **Optimizer Differentiation** â†’ AdamW for stability in BERT; Lion for fast convergence in Mistral.  
- **Duo Orchestration** â†’ a TGDK interface that dynamically routes optimizer updates across models.  
- **Hybrid Licensing** â†’ this package is released under the TGDK-BFE Research License (see [LICENSE](LICENSE.txt)).

---

## ğŸ§© Project Structure
```
â”œâ”€â”€ qlora.py # Training wrapper with multi-model support
â”œâ”€â”€ Duo.py # AdamW + Lion selection logic
â”œâ”€â”€ duo_interface.py # Public Duo arbitration interface (no internals)
â”œâ”€â”€ train_config.yaml # Example training configuration
â”œâ”€â”€ README.md # You are here
â””â”€â”€ examples/
â”œâ”€â”€ run_mistral.sh # Fine-tune Mistral with Lion
â””â”€â”€ run_bert.sh # Fine-tune BERT with AdamW
```

## âš™ï¸ Usage


AdamW â†’ BERT (encoder stability, clause fidelity).

Lion â†’ Mistral (generative speed, entropy-aware loops).

âš ï¸ Note: This repo exposes only the public Duo interface. Full Duo internals, VaultLedger hooks, and TGDK sealing remain proprietary.

## ğŸ“œ License
This release is provided under the TGDK-BFE Research License.

Free for research, academic, and non-commercial use.

Commercial deployment requires a TGDK license and VaultLedger binding.

ğŸ“¡ Credits
Developed by Sean Tichenor (TGDK LLC) with OliviaAI orchestration.
Part of the TGDK Sovereign AI Ecosystem.
